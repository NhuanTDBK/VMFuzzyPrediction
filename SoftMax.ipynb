{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "import math\n",
      "from estimators.NeuralFlow import *\n",
      "from utils.SlidingWindowUtil import SlidingWindow\n",
      "import skflow as sf\n",
      "from sklearn import datasets, metrics\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from utils.GraphUtil import *\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "scaler = MinMaxScaler()\n",
      "dat = scaler.fit_transform(pd.read_csv('sample_610_10min_unnormalize.csv',parse_dates=True,index_col=0,names=['cpu_rate'])['cpu_rate'][:3000])\n",
      "dat = pd.Series(dat.round(4))\n",
      "\n",
      "partition_size = 0.001\n",
      "umin = math.floor(min(dat));\n",
      "umax = math.ceil(max(dat));\n",
      "# 2: Partition of universe\n",
      "# Method: Dividing in the half-thousands\n",
      "def get_midpoint(ptuple):\n",
      "    return 0.5*(ptuple[0]+ptuple[1])\n",
      "def get_midpoint_vector(tuple_vector):\n",
      "    return [get_midpoint(x) for x in tuple_vector];\n",
      "def get_fuzzy_class(point, partition_size):\n",
      "    return int(math.floor(point / partition_size))\n",
      "def get_fuzzy_dataset(data):\n",
      "    u_class = []\n",
      "    for item in data:\n",
      "        u_class.append(get_fuzzy_class(item,partition_size))\n",
      "    return u_class\n",
      "def mapping_class(u_class):\n",
      "    unique_class = np.unique(u_class)\n",
      "    index = np.arange(unique_class.shape[0])\n",
      "    inverted = {}\n",
      "    mapping = {}\n",
      "    for idx,val in enumerate(unique_class):\n",
      "        mapping[val] = idx\n",
      "        inverted[idx] = val\n",
      "    return mapping, inverted\n",
      "def defuzzy(index, inverted,midpoints):\n",
      "    f_class = inverted[index]\n",
      "    return midpoints[f_class]\n",
      "\n",
      "sliding_number = 3\n",
      "# result = []\n",
      "\n",
      "nIter = int((umax-umin)/partition_size)\n",
      "u_vectorized = []\n",
      "\n",
      "for i in range(nIter) :\n",
      "    u_vectorized.append((umin + i*partition_size,umin + (i+1)*partition_size));\n",
      "\n",
      "u_midpoints = get_midpoint_vector(u_vectorized)\n",
      "u_class = np.array(get_fuzzy_dataset(dat),dtype=np.int32)\n",
      "\n",
      "onehotEncoder = OneHotEncoder()\n",
      "features = onehotEncoder.fit_transform(u_class.reshape(-1,1)).toarray()\n",
      "\n",
      "u_unique_inverted, u_unique_mapping = mapping_class(u_class)\n",
      "u_class_transform = [u_unique_inverted[item] for item in u_class]\n",
      "\n",
      "X_train_size = int(len(u_class_transform)*0.7)\n",
      "sliding = np.array(list(SlidingWindow(u_class, sliding_number)))\n",
      "sliding = np.array(sliding, dtype=np.int32)\n",
      "X_train = sliding[:X_train_size]\n",
      "y_train = features[sliding_number:X_train_size+sliding_number]\n",
      "X_test = sliding[X_train_size:]\n",
      "y_test = features[X_train_size+sliding_number-1:]\n",
      "y_actual_test = dat[X_train_size+sliding_number-1:].tolist()\n",
      "# # Define classifier\n",
      "n_hidden = len(u_unique_inverted) + sliding_number"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:324: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:359: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = tf.placeholder(tf.float32, shape=[None, len(X_train[0])])\n",
      "y_ = tf.placeholder(tf.float32, shape=[None, features.shape[1]])\n",
      "\n",
      "W = tf.Variable(tf.zeros([len(X_train[0]),features.shape[1]]))\n",
      "b = tf.Variable(tf.zeros([features.shape[1]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sess = tf.InteractiveSession()\n",
      "sess.run(tf.initialize_all_variables())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
      "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
      "train_step = tf.train.AdamOptimizer(1E-02).minimize(cross_entropy) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "(2100, 164)"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_epochs = 2000\n",
      "sess.run(tf.initialize_all_variables())\n",
      "# Training cycle\n",
      "for epoch in range(training_epochs):\n",
      "#         total_batch = int(X_train.shape[0]/batch_size)\n",
      "    # Loop over all batches\n",
      "#         for i in range(total_batch):\n",
      "#             batch_x, batch_y = X_train[i%\n",
      "        # Run optimization op (backprop) and cost op (to get loss value)\n",
      "    train_step.run(feed_dict={x: X_train,y_: y_train})\n",
      "    # Compute average loss\n",
      "#         avg_cost += c\n",
      "#         # Display logs per epoch step\n",
      "#         if epoch % display_step == 0:\n",
      "#             print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
      "#                 \"{:.9f}\".format(avg_cost)\n",
      "print \"Optimization Finished!\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization Finished!\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
      "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100\n",
      "print(sess.run(accuracy, feed_dict={x: X_test, y_: y_test}))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.445434\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}