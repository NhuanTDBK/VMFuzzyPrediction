{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapped name None to device cuda: GeForce GT 630M\n",
      "Using cuDNN version 5103 on context None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from estimators.NeuralFlow import *\n",
    "from utils.SlidingWindowUtil import SlidingWindow\n",
    "\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from utils.GraphUtil import *\n",
    "from estimators.GAEstimator import GAEstimator\n",
    "from io_utils.NumLoad import *\n",
    "from estimators.OptimizerNNEstimator import OptimizerNNEstimator\n",
    "from scaling.ProactiveSLA import ProactiveSLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2457\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "dat = pd.read_csv('sampling_617685_metric_10min_datetime.csv',parse_dates=True,index_col=0)[:3000]\n",
    "# dat = pd.read_csv('data/gdata/sampling_617685_metric_1min_datetime.csv',parse_dates=True,index_col=0)\n",
    "dat = pd.Series(dat['cpu_rate'].round(3))\n",
    "distance = round(dat.max() / (dat.max() / 0.25 + 4), 4)\n",
    "print distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Fuzzy Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partition_size = distance\n",
    "umin = math.floor(min(dat));\n",
    "umax = math.ceil(max(dat));\n",
    "# 2: Partition of universe\n",
    "# Method: Dividing in the half-thousands\n",
    "def get_midpoint(ptuple):\n",
    "    return 0.5*(ptuple[0]+ptuple[1])\n",
    "def get_midpoint_vector(tuple_vector):\n",
    "    return [get_midpoint(x) for x in tuple_vector];\n",
    "def get_fuzzy_class(point, partition_size):\n",
    "    return int(math.floor(point / partition_size))\n",
    "def get_fuzzy_dataset(data):\n",
    "    u_class = []\n",
    "    for item in data:\n",
    "        u_class.append(get_fuzzy_class(item,partition_size))\n",
    "    return u_class\n",
    "def mapping_class(u_class):\n",
    "    unique_class = np.unique(u_class)\n",
    "    index = np.arange(unique_class.shape[0])\n",
    "    inverted = {}\n",
    "    mapping = {}\n",
    "    for idx,val in enumerate(unique_class):\n",
    "        mapping[val] = idx\n",
    "        inverted[idx] = val\n",
    "    return mapping, inverted\n",
    "def defuzzy(index, inverted,midpoints):\n",
    "    f_class = inverted[index]\n",
    "    return midpoints[f_class]\n",
    "\n",
    "nIter = int((umax-umin)/partition_size)\n",
    "u_vectorized = []\n",
    "\n",
    "for i in range(nIter) :\n",
    "    u_vectorized.append((umin + i*partition_size,umin + (i+1)*partition_size));\n",
    "\n",
    "u_midpoints = get_midpoint_vector(u_vectorized)\n",
    "u_class = np.array(get_fuzzy_dataset(dat),dtype=np.int32)\n",
    "\n",
    "u_unique_inverted, u_unique_mapping = mapping_class(u_class)\n",
    "u_class_transform = [u_unique_inverted[item] for item in u_class]\n",
    "sliding_number = 3\n",
    "# result = []\n",
    "X_train_size = int(len(u_class_transform)*0.7)\n",
    "sliding = np.array(list(SlidingWindow(u_class_transform, sliding_number)))\n",
    "sliding = np.array(sliding, dtype=np.int32)\n",
    "X_train = sliding[:X_train_size]\n",
    "y_train = u_class_transform[sliding_number:X_train_size+sliding_number]\n",
    "X_test = sliding[X_train_size:]\n",
    "y_test = u_class_transform[X_train_size+sliding_number-1:]\n",
    "y_actual_test = dat[X_train_size+sliding_number-1:].tolist()\n",
    "# # Define classifier\n",
    "n_hidden = len(X_train[0]) + sliding_number\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.savez(\"fuzzy_train_direct\",X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classifier = skflow.TensorFlowDNNClassifier(hidden_units=[n_hidden], n_classes=len(u_unique_inverted),steps=10000, \n",
    "#                                             learning_rate= 0.0001, optimizer='Adam', verbose=0)\n",
    "def main_FGABPNN():   \n",
    "    fit_params = {\n",
    "        'neural_shape':[len(X_train[0]),n_hidden,1]\n",
    "        }\n",
    "    ga_estimator = GAEstimator(cross_rate=0.15, mutation_rate=0.06, gen_size=100, pop_size=30)\n",
    "    nn = NeuralFlowRegressor(hidden_nodes=[n_hidden],optimize='Adam'\n",
    "                                     ,steps=7000,learning_rate=1E-02)\n",
    "    classifier = OptimizerNNEstimator(ga_estimator, nn)\n",
    "    #     classifier = NeuralFlowRegressor(hidden_nodes=[15],optimize='Adam'\n",
    "    #                                      ,steps=7000,learning_rate=1E-03)\n",
    "    a = classifier.fit(X_train, y_train, **fit_params)\n",
    "    ypred = np.round(abs(classifier.predict(X_test))).flatten()\n",
    "    ypred_defuzzy = [defuzzy(item%len(u_unique_mapping),u_unique_mapping,u_midpoints) for item in ypred]\n",
    "    score = mean_absolute_error(ypred_defuzzy,y_actual_test)\n",
    "    # result.append((sliding_number,score))\n",
    "    print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization\n",
      "Initilization\n",
      "Gen. 0 (0.00%): Max/Min/Avg Fitness(Raw) [14.42(25.49)/10.82(5.32)/12.02(12.02)]\n",
      "Gen. 10 (33.33%): Max/Min/Avg Fitness(Raw) [20.52(37.80)/15.23(5.79)/17.10(17.10)]\n",
      "Gen. 20 (66.67%): Max/Min/Avg Fitness(Raw) [30.58(59.06)/22.68(7.04)/25.48(25.48)]\n",
      "Gen. 30 (100.00%): Max/Min/Avg Fitness(Raw) [40.80(67.96)/28.62(7.11)/34.00(34.00)]\n",
      "Total time elapsed: 1.170 seconds.\n",
      "Step #100, epoch #4, avg. train loss: 55.80948\n",
      "Step #200, epoch #9, avg. train loss: 24.20297\n",
      "Step #300, epoch #14, avg. train loss: 22.52450\n",
      "Step #400, epoch #19, avg. train loss: 24.28082\n",
      "Step #500, epoch #23, avg. train loss: 23.87231\n",
      "Step #600, epoch #28, avg. train loss: 23.33643\n",
      "Step #700, epoch #33, avg. train loss: 23.89617\n",
      "Step #800, epoch #38, avg. train loss: 22.97145\n",
      "Step #900, epoch #42, avg. train loss: 23.74812\n",
      "Step #1000, epoch #47, avg. train loss: 23.70489\n",
      "Step #1100, epoch #52, avg. train loss: 23.16350\n",
      "Step #1200, epoch #57, avg. train loss: 23.82572\n",
      "Step #1300, epoch #61, avg. train loss: 23.68386\n",
      "Step #1400, epoch #66, avg. train loss: 23.56343\n",
      "Step #1500, epoch #71, avg. train loss: 23.28934\n",
      "Step #1600, epoch #76, avg. train loss: 23.66874\n",
      "Step #1700, epoch #80, avg. train loss: 23.54157\n",
      "Step #1800, epoch #85, avg. train loss: 23.24253\n",
      "Step #1900, epoch #90, avg. train loss: 23.40953\n",
      "Step #2000, epoch #95, avg. train loss: 23.82548\n",
      "Step #2100, epoch #100, avg. train loss: 23.57343\n",
      "Step #2200, epoch #104, avg. train loss: 23.22694\n",
      "Step #2300, epoch #109, avg. train loss: 23.97458\n",
      "Step #2400, epoch #114, avg. train loss: 24.03862\n",
      "Step #2500, epoch #119, avg. train loss: 22.60137\n",
      "Step #2600, epoch #123, avg. train loss: 23.12244\n",
      "Step #2700, epoch #128, avg. train loss: 24.64952\n",
      "Step #2800, epoch #133, avg. train loss: 22.43896\n",
      "Step #2900, epoch #138, avg. train loss: 23.44148\n",
      "Step #3000, epoch #142, avg. train loss: 23.95941\n",
      "Step #3100, epoch #147, avg. train loss: 22.76194\n",
      "Step #3200, epoch #152, avg. train loss: 24.38397\n",
      "Step #3300, epoch #157, avg. train loss: 23.21861\n",
      "Step #3400, epoch #161, avg. train loss: 23.51250\n",
      "Step #3500, epoch #166, avg. train loss: 23.30472\n",
      "Step #3600, epoch #171, avg. train loss: 23.55680\n",
      "Step #3700, epoch #176, avg. train loss: 23.27204\n",
      "Step #3800, epoch #180, avg. train loss: 23.98007\n",
      "Step #3900, epoch #185, avg. train loss: 22.90262\n",
      "Step #4000, epoch #190, avg. train loss: 23.96420\n",
      "Step #4100, epoch #195, avg. train loss: 23.48968\n",
      "Step #4200, epoch #200, avg. train loss: 23.40213\n",
      "Step #4300, epoch #204, avg. train loss: 24.00050\n",
      "Step #4400, epoch #209, avg. train loss: 22.90414\n",
      "Step #4500, epoch #214, avg. train loss: 24.03919\n",
      "Step #4600, epoch #219, avg. train loss: 22.78721\n",
      "Step #4700, epoch #223, avg. train loss: 23.90141\n",
      "Step #4800, epoch #228, avg. train loss: 22.97889\n",
      "Step #4900, epoch #233, avg. train loss: 23.57310\n",
      "Step #5000, epoch #238, avg. train loss: 23.51981\n",
      "Step #5100, epoch #242, avg. train loss: 22.85800\n",
      "Step #5200, epoch #247, avg. train loss: 24.00278\n",
      "Step #5300, epoch #252, avg. train loss: 23.28497\n",
      "Step #5400, epoch #257, avg. train loss: 23.87902\n",
      "Step #5500, epoch #261, avg. train loss: 23.44624\n",
      "Step #5600, epoch #266, avg. train loss: 23.38828\n",
      "Step #5700, epoch #271, avg. train loss: 24.21593\n",
      "Step #5800, epoch #276, avg. train loss: 22.89634\n",
      "Step #5900, epoch #280, avg. train loss: 23.41451\n",
      "Step #6000, epoch #285, avg. train loss: 23.81667\n",
      "Step #6100, epoch #290, avg. train loss: 23.60014\n",
      "Step #6200, epoch #295, avg. train loss: 22.70227\n",
      "Step #6300, epoch #300, avg. train loss: 23.60551\n",
      "Step #6400, epoch #304, avg. train loss: 23.85645\n",
      "Step #6500, epoch #309, avg. train loss: 22.87661\n",
      "Step #6600, epoch #314, avg. train loss: 23.13413\n",
      "Step #6700, epoch #319, avg. train loss: 23.98222\n",
      "Step #6800, epoch #323, avg. train loss: 23.09640\n",
      "Step #6900, epoch #328, avg. train loss: 24.46483\n",
      "Step #7000, epoch #333, avg. train loss: 22.93399\n",
      "0.886919599109\n",
      "Initialization\n",
      "Initilization\n",
      "Gen. 0 (0.00%): Max/Min/Avg Fitness(Raw) [11.64(22.95)/9.14(5.91)/9.70(9.70)]\n",
      "Gen. 10 (33.33%): Max/Min/Avg Fitness(Raw) [20.15(39.92)/15.62(8.74)/16.79(16.79)]\n",
      "Gen. 20 (66.67%): Max/Min/Avg Fitness(Raw) [29.82(44.68)/20.52(7.58)/24.85(24.85)]\n",
      "Gen. 30 (100.00%): Max/Min/Avg Fitness(Raw) [42.42(54.45)/25.08(7.61)/35.35(35.35)]\n",
      "Total time elapsed: 0.726 seconds.\n",
      "Step #100, epoch #4, avg. train loss: 40.68661\n",
      "Step #200, epoch #9, avg. train loss: 24.35702\n",
      "Step #300, epoch #14, avg. train loss: 22.56206\n",
      "Step #400, epoch #19, avg. train loss: 24.39974\n",
      "Step #500, epoch #23, avg. train loss: 23.98145\n",
      "Step #600, epoch #28, avg. train loss: 23.34539\n",
      "Step #700, epoch #33, avg. train loss: 23.96647\n",
      "Step #800, epoch #38, avg. train loss: 23.08880\n",
      "Step #900, epoch #42, avg. train loss: 23.79129\n",
      "Step #1000, epoch #47, avg. train loss: 23.75528\n",
      "Step #1100, epoch #52, avg. train loss: 23.15922\n",
      "Step #1200, epoch #57, avg. train loss: 23.74549\n",
      "Step #1300, epoch #61, avg. train loss: 23.64929\n",
      "Step #1400, epoch #66, avg. train loss: 23.42725\n",
      "Step #1500, epoch #71, avg. train loss: 23.12225\n",
      "Step #1600, epoch #76, avg. train loss: 23.40009\n",
      "Step #1700, epoch #80, avg. train loss: 23.21719\n",
      "Step #1800, epoch #85, avg. train loss: 22.97665\n",
      "Step #1900, epoch #90, avg. train loss: 23.20502\n",
      "Step #2000, epoch #95, avg. train loss: 23.45315\n",
      "Step #2100, epoch #100, avg. train loss: 23.21868\n",
      "Step #2200, epoch #104, avg. train loss: 22.91552\n",
      "Step #2300, epoch #109, avg. train loss: 23.57207\n",
      "Step #2400, epoch #114, avg. train loss: 23.76101\n",
      "Step #2500, epoch #119, avg. train loss: 22.29597\n",
      "Step #2600, epoch #123, avg. train loss: 22.75821\n",
      "Step #2700, epoch #128, avg. train loss: 24.18184\n",
      "Step #2800, epoch #133, avg. train loss: 22.06343\n",
      "Step #2900, epoch #138, avg. train loss: 23.06576\n",
      "Step #3000, epoch #142, avg. train loss: 23.45285\n",
      "Step #3100, epoch #147, avg. train loss: 22.31830\n",
      "Step #3200, epoch #152, avg. train loss: 24.02543\n",
      "Step #3300, epoch #157, avg. train loss: 22.85394\n",
      "Step #3400, epoch #161, avg. train loss: 22.96892\n",
      "Step #3500, epoch #166, avg. train loss: 22.93662\n",
      "Step #3600, epoch #171, avg. train loss: 23.19131\n",
      "Step #3700, epoch #176, avg. train loss: 22.84958\n",
      "Step #3800, epoch #180, avg. train loss: 23.53111\n",
      "Step #3900, epoch #185, avg. train loss: 22.59938\n",
      "Step #4000, epoch #190, avg. train loss: 23.49851\n",
      "Step #4100, epoch #195, avg. train loss: 23.00554\n",
      "Step #4200, epoch #200, avg. train loss: 22.96522\n",
      "Step #4300, epoch #204, avg. train loss: 23.47177\n",
      "Step #4400, epoch #209, avg. train loss: 22.40964\n",
      "Step #4500, epoch #214, avg. train loss: 23.67350\n",
      "Step #4600, epoch #219, avg. train loss: 22.37580\n",
      "Step #4700, epoch #223, avg. train loss: 23.45645\n",
      "Step #4800, epoch #228, avg. train loss: 22.58519\n",
      "Step #4900, epoch #233, avg. train loss: 23.05958\n",
      "Step #5000, epoch #238, avg. train loss: 23.18360\n",
      "Step #5100, epoch #242, avg. train loss: 22.52753\n",
      "Step #5200, epoch #247, avg. train loss: 23.48533\n",
      "Step #5300, epoch #252, avg. train loss: 22.82653\n",
      "Step #5400, epoch #257, avg. train loss: 23.47702\n",
      "Step #5500, epoch #261, avg. train loss: 22.98807\n",
      "Step #5600, epoch #266, avg. train loss: 23.05171\n",
      "Step #5700, epoch #271, avg. train loss: 23.73263\n",
      "Step #5800, epoch #276, avg. train loss: 22.55360\n",
      "Step #5900, epoch #280, avg. train loss: 22.87407\n",
      "Step #6000, epoch #285, avg. train loss: 23.38352\n",
      "Step #6100, epoch #290, avg. train loss: 23.10332\n",
      "Step #6200, epoch #295, avg. train loss: 22.31566\n",
      "Step #6300, epoch #300, avg. train loss: 23.21806\n",
      "Step #6400, epoch #304, avg. train loss: 23.51363\n",
      "Step #6500, epoch #309, avg. train loss: 22.45154\n",
      "Step #6600, epoch #314, avg. train loss: 22.70901\n",
      "Step #6700, epoch #319, avg. train loss: 23.54415\n",
      "Step #6800, epoch #323, avg. train loss: 22.58516\n",
      "Step #6900, epoch #328, avg. train loss: 24.11295\n",
      "Step #7000, epoch #333, avg. train loss: 22.50764\n",
      "0.869416926503\n",
      "Initialization\n",
      "Initilization\n",
      "Gen. 0 (0.00%): Max/Min/Avg Fitness(Raw) [10.57(12.78)/7.43(5.71)/8.81(8.81)]\n",
      "Gen. 10 (33.33%): Max/Min/Avg Fitness(Raw) [19.06(32.71)/14.17(6.80)/15.88(15.88)]\n",
      "Gen. 20 (66.67%): Max/Min/Avg Fitness(Raw) [23.49(33.72)/16.39(8.09)/19.57(19.57)]\n",
      "Gen. 30 (100.00%): Max/Min/Avg Fitness(Raw) [36.83(54.07)/26.34(14.11)/30.69(30.69)]\n",
      "Total time elapsed: 0.906 seconds.\n",
      "Step #100, epoch #4, avg. train loss: 40.87595\n",
      "Step #200, epoch #9, avg. train loss: 23.99104\n",
      "Step #300, epoch #14, avg. train loss: 22.09346\n",
      "Step #400, epoch #19, avg. train loss: 23.80777\n",
      "Step #500, epoch #23, avg. train loss: 23.49622\n",
      "Step #600, epoch #28, avg. train loss: 22.68828\n",
      "Step #700, epoch #33, avg. train loss: 23.29742\n",
      "Step #800, epoch #38, avg. train loss: 22.28114\n",
      "Step #900, epoch #42, avg. train loss: 22.66773\n",
      "Step #1000, epoch #47, avg. train loss: 22.80813\n",
      "Step #1100, epoch #52, avg. train loss: 22.32496\n",
      "Step #1200, epoch #57, avg. train loss: 22.71995\n",
      "Step #1300, epoch #61, avg. train loss: 22.75340\n",
      "Step #1400, epoch #66, avg. train loss: 22.61346\n",
      "Step #1500, epoch #71, avg. train loss: 22.18173\n",
      "Step #1600, epoch #76, avg. train loss: 22.59490\n",
      "Step #1700, epoch #80, avg. train loss: 22.50667\n",
      "Step #1800, epoch #85, avg. train loss: 22.33634\n",
      "Step #1900, epoch #90, avg. train loss: 22.15063\n",
      "Step #2000, epoch #95, avg. train loss: 22.73795\n",
      "Step #2100, epoch #100, avg. train loss: 22.35299\n",
      "Step #2200, epoch #104, avg. train loss: 22.06349\n",
      "Step #2300, epoch #109, avg. train loss: 22.86103\n",
      "Step #2400, epoch #114, avg. train loss: 22.93839\n",
      "Step #2500, epoch #119, avg. train loss: 21.50576\n",
      "Step #2600, epoch #123, avg. train loss: 22.19927\n",
      "Step #2700, epoch #128, avg. train loss: 23.47594\n",
      "Step #2800, epoch #133, avg. train loss: 21.65925\n",
      "Step #2900, epoch #138, avg. train loss: 22.19551\n",
      "Step #3000, epoch #142, avg. train loss: 22.76473\n",
      "Step #3100, epoch #147, avg. train loss: 21.66412\n",
      "Step #3200, epoch #152, avg. train loss: 23.27509\n",
      "Step #3300, epoch #157, avg. train loss: 22.00401\n",
      "Step #3400, epoch #161, avg. train loss: 22.32316\n",
      "Step #3500, epoch #166, avg. train loss: 22.20673\n",
      "Step #3600, epoch #171, avg. train loss: 22.56882\n",
      "Step #3700, epoch #176, avg. train loss: 22.19628\n",
      "Step #3800, epoch #180, avg. train loss: 22.86797\n",
      "Step #3900, epoch #185, avg. train loss: 21.85559\n",
      "Step #4000, epoch #190, avg. train loss: 22.88046\n",
      "Step #4100, epoch #195, avg. train loss: 22.29613\n",
      "Step #4200, epoch #200, avg. train loss: 22.25095\n",
      "Step #4300, epoch #204, avg. train loss: 22.83975\n",
      "Step #4400, epoch #209, avg. train loss: 21.82295\n",
      "Step #4500, epoch #214, avg. train loss: 22.91930\n",
      "Step #4600, epoch #219, avg. train loss: 21.66533\n",
      "Step #4700, epoch #223, avg. train loss: 22.84441\n",
      "Step #4800, epoch #228, avg. train loss: 21.93842\n",
      "Step #4900, epoch #233, avg. train loss: 22.39222\n",
      "Step #5000, epoch #238, avg. train loss: 22.37233\n",
      "Step #5100, epoch #242, avg. train loss: 21.69999\n",
      "Step #5200, epoch #247, avg. train loss: 23.00012\n",
      "Step #5300, epoch #252, avg. train loss: 21.94480\n",
      "Step #5400, epoch #257, avg. train loss: 22.80959\n",
      "Step #5500, epoch #261, avg. train loss: 22.32396\n",
      "Step #5600, epoch #266, avg. train loss: 22.06786\n",
      "Step #5700, epoch #271, avg. train loss: 23.06645\n",
      "Step #5800, epoch #276, avg. train loss: 21.57252\n",
      "Step #5900, epoch #280, avg. train loss: 21.98428\n",
      "Step #6000, epoch #285, avg. train loss: 22.65676\n",
      "Step #6100, epoch #290, avg. train loss: 22.22979\n",
      "Step #6200, epoch #295, avg. train loss: 21.61888\n",
      "Step #6300, epoch #300, avg. train loss: 22.34003\n",
      "Step #6400, epoch #304, avg. train loss: 22.66183\n",
      "Step #6500, epoch #309, avg. train loss: 21.47581\n",
      "Step #6600, epoch #314, avg. train loss: 21.95881\n",
      "Step #6700, epoch #319, avg. train loss: 22.73923\n",
      "Step #6800, epoch #323, avg. train loss: 21.85070\n",
      "Step #6900, epoch #328, avg. train loss: 23.04811\n",
      "Step #7000, epoch #333, avg. train loss: 21.86015\n",
      "0.730796993318\n",
      "Initialization\n",
      "Initilization\n",
      "Gen. 0 (0.00%): Max/Min/Avg Fitness(Raw) [11.94(20.38)/9.25(6.27)/9.95(9.95)]\n",
      "Gen. 10 (33.33%): Max/Min/Avg Fitness(Raw) [17.92(39.56)/13.90(6.41)/14.93(14.93)]\n",
      "Gen. 20 (66.67%): Max/Min/Avg Fitness(Raw) [22.54(47.90)/17.28(7.11)/18.78(18.78)]\n",
      "Gen. 30 (100.00%): Max/Min/Avg Fitness(Raw) [38.84(59.46)/26.82(9.16)/32.37(32.37)]\n",
      "Total time elapsed: 1.083 seconds.\n",
      "Step #100, epoch #4, avg. train loss: 45.37798\n",
      "Step #200, epoch #9, avg. train loss: 24.32168\n",
      "Step #300, epoch #14, avg. train loss: 22.40832\n",
      "Step #400, epoch #19, avg. train loss: 24.29936\n",
      "Step #500, epoch #23, avg. train loss: 23.98247\n",
      "Step #600, epoch #28, avg. train loss: 23.37085\n",
      "Step #700, epoch #33, avg. train loss: 23.93291\n",
      "Step #800, epoch #38, avg. train loss: 22.96405\n",
      "Step #900, epoch #42, avg. train loss: 23.71124\n",
      "Step #1000, epoch #47, avg. train loss: 23.61140\n",
      "Step #1100, epoch #52, avg. train loss: 23.11930\n",
      "Step #1200, epoch #57, avg. train loss: 23.69945\n",
      "Step #1300, epoch #61, avg. train loss: 23.63931\n",
      "Step #1400, epoch #66, avg. train loss: 23.48935\n",
      "Step #1500, epoch #71, avg. train loss: 23.14432\n",
      "Step #1600, epoch #76, avg. train loss: 23.41644\n",
      "Step #1700, epoch #80, avg. train loss: 23.34689\n",
      "Step #1800, epoch #85, avg. train loss: 23.16584\n",
      "Step #1900, epoch #90, avg. train loss: 23.31999\n",
      "Step #2000, epoch #95, avg. train loss: 23.60775\n",
      "Step #2100, epoch #100, avg. train loss: 23.41450\n",
      "Step #2200, epoch #104, avg. train loss: 23.15287\n",
      "Step #2300, epoch #109, avg. train loss: 23.81994\n",
      "Step #2400, epoch #114, avg. train loss: 23.95832\n",
      "Step #2500, epoch #119, avg. train loss: 22.44344\n",
      "Step #2600, epoch #123, avg. train loss: 23.09510\n",
      "Step #2700, epoch #128, avg. train loss: 24.43321\n",
      "Step #2800, epoch #133, avg. train loss: 22.39552\n",
      "Step #2900, epoch #138, avg. train loss: 23.23335\n",
      "Step #3000, epoch #142, avg. train loss: 23.78422\n",
      "Step #3100, epoch #147, avg. train loss: 22.70186\n",
      "Step #3200, epoch #152, avg. train loss: 24.30843\n",
      "Step #3300, epoch #157, avg. train loss: 23.17921\n",
      "Step #3400, epoch #161, avg. train loss: 23.35642\n",
      "Step #3500, epoch #166, avg. train loss: 23.18952\n",
      "Step #3600, epoch #171, avg. train loss: 23.50873\n",
      "Step #3700, epoch #176, avg. train loss: 23.19199\n",
      "Step #3800, epoch #180, avg. train loss: 23.86225\n",
      "Step #3900, epoch #185, avg. train loss: 22.81577\n",
      "Step #4000, epoch #190, avg. train loss: 23.87623\n",
      "Step #4100, epoch #195, avg. train loss: 23.29974\n",
      "Step #4200, epoch #200, avg. train loss: 23.26658\n",
      "Step #4300, epoch #204, avg. train loss: 23.84066\n",
      "Step #4400, epoch #209, avg. train loss: 22.81259\n",
      "Step #4500, epoch #214, avg. train loss: 23.99651\n",
      "Step #4600, epoch #219, avg. train loss: 22.62937\n",
      "Step #4700, epoch #223, avg. train loss: 23.76165\n",
      "Step #4800, epoch #228, avg. train loss: 22.82777\n",
      "Step #4900, epoch #233, avg. train loss: 23.38407\n",
      "Step #5000, epoch #238, avg. train loss: 23.39098\n",
      "Step #5100, epoch #242, avg. train loss: 22.78733\n",
      "Step #5200, epoch #247, avg. train loss: 23.75923\n",
      "Step #5300, epoch #252, avg. train loss: 23.19868\n",
      "Step #5400, epoch #257, avg. train loss: 23.79800\n",
      "Step #5500, epoch #261, avg. train loss: 23.30255\n",
      "Step #5600, epoch #266, avg. train loss: 23.26826\n",
      "Step #5700, epoch #271, avg. train loss: 24.03775\n",
      "Step #5800, epoch #276, avg. train loss: 22.76343\n",
      "Step #5900, epoch #280, avg. train loss: 23.27422\n",
      "Step #6000, epoch #285, avg. train loss: 23.66592\n",
      "Step #6100, epoch #290, avg. train loss: 23.47677\n",
      "Step #6200, epoch #295, avg. train loss: 22.61623\n",
      "Step #6300, epoch #300, avg. train loss: 23.43081\n",
      "Step #6400, epoch #304, avg. train loss: 23.78524\n",
      "Step #6500, epoch #309, avg. train loss: 22.69100\n",
      "Step #6600, epoch #314, avg. train loss: 22.88404\n",
      "Step #6700, epoch #319, avg. train loss: 23.74967\n",
      "Step #6800, epoch #323, avg. train loss: 22.75521\n",
      "Step #6900, epoch #328, avg. train loss: 24.25645\n",
      "Step #7000, epoch #333, avg. train loss: 22.70188\n",
      "0.879535634744\n",
      "1 loops, best of 3: 9.82 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "main_FGABPNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The set_color_cycle attribute was deprecated in version 1.5. Use set_prop_cycle instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "plot_figure(y_pred=ypred_defuzzy[:200],y_true=y_actual_test[:200], title='High Order Time Series with order %s, hidden nodes = %s: %s'%(sliding_number,n_hidden,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.79064587973\n",
      "Using matplotlib backend: Qt4Agg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f18777bfb50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = ProactiveSLA(sla=1.5, past_consecutive_values=3)\n",
    "predict_allocated = scaler.allocate_VMs(np.array(y_actual_test), ypred_defuzzy)\n",
    "actual_allocated = np.array([scaler.allocate_VM(item) for item in np.array(y_actual_test)])\n",
    "delta = actual_allocated - predict_allocated\n",
    "print float(len(delta[delta>0])) / len(predict_allocated) *100\n",
    "%matplotlib\n",
    "ax = plt.subplot()\n",
    "ax.plot(predict_allocated[100:300],'--',label='Predict Allocated')\n",
    "ax.plot(actual_allocated[100:300],label='Actual Allocated')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Number of VMs')\n",
    "ax.set_title(\"High Order (sliding window 3) Fuzzy by CPU metric\")\n",
    "# ax.plot(trainee_holder['cpu_rate']['y_test'],label='Resource Used')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savez('fuzzy_GABPNN_%s_%s'%(sliding_number,score),y_pred=ypred_defuzzy,y_true=y_actual_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPNN Experiment with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:324: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:359: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "dat_nn = np.asarray(scaler.fit_transform(dat))\n",
    "X_train_size = int(len(dat_nn)*0.7)\n",
    "sliding = np.array(list(SlidingWindow(dat_nn, sliding_number)))\n",
    "X_train_nn = sliding[:X_train_size]\n",
    "y_train_nn = dat_nn[sliding_number:X_train_size+sliding_number].reshape(-1,1)\n",
    "X_test_nn = sliding[X_train_size:]\n",
    "y_test_nn = dat_nn[X_train_size+sliding_number-1:].reshape(-1,1)\n",
    "y_actual_test = dat[X_train_size+sliding_number-1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # def main_BPNN():\n",
    "# estimator = NeuralFlowRegressor(learning_rate=0.001, hidden_nodes=[15], steps=5000,optimize='Adam')\n",
    "# estimator.fit(X_train_nn,y_train_nn)\n",
    "# y_pred = scaler.inverse_transform(estimator.predict(X_test_nn))\n",
    "# score_nn = mean_absolute_error(y_pred,y_actual_test)\n",
    "# print score_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "plot_figure(y_pred=y_pred,y_true=y_actual_test, title='BPNN with sliding window = %s: %s'%(sliding_number,score_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.01336302895\n",
      "Using matplotlib backend: Qt4Agg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f18242cc950>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = ProactiveSLA(sla=1.5, past_consecutive_values=3)\n",
    "predict_allocated = scaler.allocate_VMs(np.array(y_actual_test), y_pred)\n",
    "actual_allocated = np.array([scaler.allocate_VM(item) for item in np.array(y_actual_test)])\n",
    "delta = actual_allocated - predict_allocated\n",
    "print float(len(delta[delta>0])) / len(predict_allocated) *100\n",
    "%matplotlib\n",
    "ax = plt.subplot()\n",
    "ax.plot(predict_allocated[100:300],'--',label='Predict Allocated')\n",
    "ax.plot(actual_allocated[100:300],label='Actual Allocated')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Number of VMs')\n",
    "ax.set_title(\"High Order (sliding window 3) Fuzzy by CPU metric\")\n",
    "# ax.plot(trainee_holder['cpu_rate']['y_test'],label='Resource Used')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savez('neuro_%s_%s'%(sliding_number,score_nn),y_pred=y_pred,y_true=y_actual_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GABPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main_GABPNN():\n",
    "    fit_params = {\n",
    "        'neural_shape':[len(X_train_nn[0]),10,1]\n",
    "        }\n",
    "    ga_estimator = GAEstimator(cross_rate=0.95, mutation_rate=0.05, gen_size=100, pop_size=30)\n",
    "    nn = NeuralFlowRegressor(hidden_nodes=[10],optimize='Adam'\n",
    "                                     ,steps=7000,learning_rate=1E-02)\n",
    "    classifier = OptimizerNNEstimator(ga_estimator, nn)\n",
    "    classifier.fit(X_train_nn, y_train_nn,**fit_params)\n",
    "    y_pred = scaler.inverse_transform(classifier.predict(X_test_nn))\n",
    "    score_nn = mean_absolute_error(y_pred,y_actual_test)\n",
    "    print score_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization\n",
      "Initilization\n",
      "Gen. 0 (0.00%): Max/Min/Avg Fitness(Raw) [1.12(2.63)/0.84(0.05)/0.94(0.94)]\n",
      "Gen. 10 (33.33%): Max/Min/Avg Fitness(Raw) [1.48(4.32)/1.14(0.07)/1.24(1.24)]\n",
      "Gen. 20 (66.67%): Max/Min/Avg Fitness(Raw) [4.05(7.51)/3.01(1.16)/3.37(3.37)]\n",
      "Gen. 30 (100.00%): Max/Min/Avg Fitness(Raw) [5.23(7.68)/3.79(2.20)/4.36(4.36)]\n",
      "Total time elapsed: 0.765 seconds.\n",
      "Step #100, epoch #4, avg. train loss: 2.55504\n",
      "Step #200, epoch #9, avg. train loss: 0.00085\n",
      "Step #300, epoch #14, avg. train loss: 0.00067\n",
      "Step #400, epoch #19, avg. train loss: 0.00071\n",
      "Step #500, epoch #23, avg. train loss: 0.00069\n",
      "Step #600, epoch #28, avg. train loss: 0.00064\n",
      "Step #700, epoch #33, avg. train loss: 0.00067\n",
      "Step #800, epoch #38, avg. train loss: 0.00063\n",
      "Step #900, epoch #42, avg. train loss: 0.00064\n",
      "Step #1000, epoch #47, avg. train loss: 0.00065\n",
      "Step #1100, epoch #52, avg. train loss: 0.00063\n",
      "Step #1200, epoch #57, avg. train loss: 0.00064\n",
      "Step #1300, epoch #61, avg. train loss: 0.00064\n",
      "Step #1400, epoch #66, avg. train loss: 0.00065\n",
      "Step #1500, epoch #71, avg. train loss: 0.00063\n",
      "Step #1600, epoch #76, avg. train loss: 0.00064\n",
      "Step #1700, epoch #80, avg. train loss: 0.00064\n",
      "Step #1800, epoch #85, avg. train loss: 0.00062\n",
      "Step #1900, epoch #90, avg. train loss: 0.00063\n",
      "Step #2000, epoch #95, avg. train loss: 0.00065\n",
      "Step #2100, epoch #100, avg. train loss: 0.00063\n",
      "Step #2200, epoch #104, avg. train loss: 0.00062\n",
      "Step #2300, epoch #109, avg. train loss: 0.00065\n",
      "Step #2400, epoch #114, avg. train loss: 0.00064\n",
      "Step #2500, epoch #119, avg. train loss: 0.00061\n",
      "Step #2600, epoch #123, avg. train loss: 0.00062\n",
      "Step #2700, epoch #128, avg. train loss: 0.00067\n",
      "Step #2800, epoch #133, avg. train loss: 0.00059\n",
      "Step #2900, epoch #138, avg. train loss: 0.00063\n",
      "Step #3000, epoch #142, avg. train loss: 0.00064\n",
      "Step #3100, epoch #147, avg. train loss: 0.00060\n",
      "Step #3200, epoch #152, avg. train loss: 0.00065\n",
      "Step #3300, epoch #157, avg. train loss: 0.00061\n",
      "Step #3400, epoch #161, avg. train loss: 0.00063\n",
      "Step #3500, epoch #166, avg. train loss: 0.00061\n",
      "Step #3600, epoch #171, avg. train loss: 0.00062\n",
      "Step #3700, epoch #176, avg. train loss: 0.00061\n",
      "Step #3800, epoch #180, avg. train loss: 0.00063\n",
      "Step #3900, epoch #185, avg. train loss: 0.00060\n",
      "Step #4000, epoch #190, avg. train loss: 0.00063\n",
      "Step #4100, epoch #195, avg. train loss: 0.00061\n",
      "Step #4200, epoch #200, avg. train loss: 0.00061\n",
      "Step #4300, epoch #204, avg. train loss: 0.00062\n",
      "Step #4400, epoch #209, avg. train loss: 0.00058\n",
      "Step #4500, epoch #214, avg. train loss: 0.00064\n",
      "Step #4600, epoch #219, avg. train loss: 0.00060\n",
      "Step #4700, epoch #223, avg. train loss: 0.00063\n",
      "Step #4800, epoch #228, avg. train loss: 0.00059\n",
      "Step #4900, epoch #233, avg. train loss: 0.00060\n",
      "Step #5000, epoch #238, avg. train loss: 0.00060\n",
      "Step #5100, epoch #242, avg. train loss: 0.00058\n",
      "Step #5200, epoch #247, avg. train loss: 0.00061\n",
      "Step #5300, epoch #252, avg. train loss: 0.00058\n",
      "Step #5400, epoch #257, avg. train loss: 0.00060\n",
      "Step #5500, epoch #261, avg. train loss: 0.00059\n",
      "Step #5600, epoch #266, avg. train loss: 0.00059\n",
      "Step #5700, epoch #271, avg. train loss: 0.00061\n",
      "Step #5800, epoch #276, avg. train loss: 0.00057\n",
      "Step #5900, epoch #280, avg. train loss: 0.00058\n",
      "Step #6000, epoch #285, avg. train loss: 0.00059\n",
      "Step #6100, epoch #290, avg. train loss: 0.00059\n",
      "Step #6200, epoch #295, avg. train loss: 0.00057\n",
      "Step #6300, epoch #300, avg. train loss: 0.00059\n",
      "Step #6400, epoch #304, avg. train loss: 0.00060\n",
      "Step #6500, epoch #309, avg. train loss: 0.00057\n",
      "Step #6600, epoch #314, avg. train loss: 0.00057\n",
      "Step #6700, epoch #319, avg. train loss: 0.00060\n",
      "Step #6800, epoch #323, avg. train loss: 0.00060\n",
      "Step #6900, epoch #328, avg. train loss: 0.00060\n",
      "Step #7000, epoch #333, avg. train loss: 0.00057\n",
      "0.965293988302\n",
      "Initialization\n",
      "Initilization\n",
      "Gen. 0 (0.00%): Max/Min/Avg Fitness(Raw) [1.47(3.19)/1.08(0.07)/1.23(1.23)]\n",
      "Gen. 10 (33.33%): Max/Min/Avg Fitness(Raw) [2.32(5.32)/1.75(0.33)/1.94(1.94)]\n",
      "Gen. 20 (66.67%): Max/Min/Avg Fitness(Raw) [3.42(6.59)/2.45(0.22)/2.85(2.85)]\n",
      "Gen. 30 (100.00%): Max/Min/Avg Fitness(Raw) [5.62(8.07)/3.71(1.15)/4.69(4.69)]\n",
      "Total time elapsed: 0.752 seconds.\n",
      "Step #100, epoch #4, avg. train loss: 2.65940\n",
      "Step #200, epoch #9, avg. train loss: 0.00138\n",
      "Step #300, epoch #14, avg. train loss: 0.00089\n",
      "Step #400, epoch #19, avg. train loss: 0.00079\n",
      "Step #500, epoch #23, avg. train loss: 0.00072\n",
      "Step #600, epoch #28, avg. train loss: 0.00065\n",
      "Step #700, epoch #33, avg. train loss: 0.00067\n",
      "Step #800, epoch #38, avg. train loss: 0.00063\n",
      "Step #900, epoch #42, avg. train loss: 0.00064\n",
      "Step #1000, epoch #47, avg. train loss: 0.00065\n",
      "Step #1100, epoch #52, avg. train loss: 0.00063\n",
      "Step #1200, epoch #57, avg. train loss: 0.00064\n",
      "Step #1300, epoch #61, avg. train loss: 0.00063\n",
      "Step #1400, epoch #66, avg. train loss: 0.00064\n",
      "Step #1500, epoch #71, avg. train loss: 0.00063\n",
      "Step #1600, epoch #76, avg. train loss: 0.00063\n",
      "Step #1700, epoch #80, avg. train loss: 0.00063\n",
      "Step #1800, epoch #85, avg. train loss: 0.00062\n",
      "Step #1900, epoch #90, avg. train loss: 0.00063\n",
      "Step #2000, epoch #95, avg. train loss: 0.00064\n",
      "Step #2100, epoch #100, avg. train loss: 0.00062\n",
      "Step #2200, epoch #104, avg. train loss: 0.00061\n",
      "Step #2300, epoch #109, avg. train loss: 0.00064\n",
      "Step #2400, epoch #114, avg. train loss: 0.00063\n",
      "Step #2500, epoch #119, avg. train loss: 0.00059\n",
      "Step #2600, epoch #123, avg. train loss: 0.00061\n",
      "Step #2700, epoch #128, avg. train loss: 0.00066\n",
      "Step #2800, epoch #133, avg. train loss: 0.00058\n",
      "Step #2900, epoch #138, avg. train loss: 0.00062\n",
      "Step #3000, epoch #142, avg. train loss: 0.00063\n",
      "Step #3100, epoch #147, avg. train loss: 0.00059\n",
      "Step #3200, epoch #152, avg. train loss: 0.00064\n",
      "Step #3300, epoch #157, avg. train loss: 0.00060\n",
      "Step #3400, epoch #161, avg. train loss: 0.00063\n",
      "Step #3500, epoch #166, avg. train loss: 0.00061\n",
      "Step #3600, epoch #171, avg. train loss: 0.00061\n",
      "Step #3700, epoch #176, avg. train loss: 0.00060\n",
      "Step #3800, epoch #180, avg. train loss: 0.00062\n",
      "Step #3900, epoch #185, avg. train loss: 0.00060\n",
      "Step #4000, epoch #190, avg. train loss: 0.00063\n",
      "Step #4100, epoch #195, avg. train loss: 0.00060\n",
      "Step #4200, epoch #200, avg. train loss: 0.00060\n",
      "Step #4300, epoch #204, avg. train loss: 0.00062\n",
      "Step #4400, epoch #209, avg. train loss: 0.00058\n",
      "Step #4500, epoch #214, avg. train loss: 0.00065\n",
      "Step #4600, epoch #219, avg. train loss: 0.00060\n",
      "Step #4700, epoch #223, avg. train loss: 0.00065\n",
      "Step #4800, epoch #228, avg. train loss: 0.00059\n",
      "Step #4900, epoch #233, avg. train loss: 0.00060\n",
      "Step #5000, epoch #238, avg. train loss: 0.00060\n",
      "Step #5100, epoch #242, avg. train loss: 0.00058\n",
      "Step #5200, epoch #247, avg. train loss: 0.00061\n",
      "Step #5300, epoch #252, avg. train loss: 0.00059\n",
      "Step #5400, epoch #257, avg. train loss: 0.00061\n",
      "Step #5500, epoch #261, avg. train loss: 0.00060\n",
      "Step #5600, epoch #266, avg. train loss: 0.00059\n",
      "Step #5700, epoch #271, avg. train loss: 0.00064\n",
      "Step #5800, epoch #276, avg. train loss: 0.00059\n",
      "Step #5900, epoch #280, avg. train loss: 0.00061\n",
      "Step #6000, epoch #285, avg. train loss: 0.00060\n",
      "Step #6100, epoch #290, avg. train loss: 0.00060\n",
      "Step #6200, epoch #295, avg. train loss: 0.00060\n",
      "Step #6300, epoch #300, avg. train loss: 0.00060\n",
      "Step #6400, epoch #304, avg. train loss: 0.00062\n",
      "Step #6500, epoch #309, avg. train loss: 0.00058\n",
      "Step #6600, epoch #314, avg. train loss: 0.00059\n",
      "Step #6700, epoch #319, avg. train loss: 0.00061\n",
      "Step #6800, epoch #323, avg. train loss: 0.00062\n",
      "Step #6900, epoch #328, avg. train loss: 0.00063\n",
      "Step #7000, epoch #333, avg. train loss: 0.00060\n",
      "1.00268221713\n",
      "Initialization\n",
      "Initilization\n",
      "Gen. 0 (0.00%): Max/Min/Avg Fitness(Raw) [1.08(2.75)/0.81(0.05)/0.90(0.90)]\n",
      "Gen. 10 (33.33%): Max/Min/Avg Fitness(Raw) [2.19(5.24)/1.64(0.13)/1.82(1.82)]\n",
      "Gen. 20 (66.67%): Max/Min/Avg Fitness(Raw) [2.31(5.79)/1.76(0.27)/1.93(1.93)]\n",
      "Gen. 30 (100.00%): Max/Min/Avg Fitness(Raw) [3.88(7.16)/2.77(0.40)/3.23(3.23)]\n",
      "Total time elapsed: 1.039 seconds.\n",
      "Step #100, epoch #4, avg. train loss: 2.34984\n",
      "Step #200, epoch #9, avg. train loss: 0.00079\n",
      "Step #300, epoch #14, avg. train loss: 0.00063\n",
      "Step #400, epoch #19, avg. train loss: 0.00068\n",
      "Step #500, epoch #23, avg. train loss: 0.00067\n",
      "Step #600, epoch #28, avg. train loss: 0.00063\n",
      "Step #700, epoch #33, avg. train loss: 0.00065\n",
      "Step #800, epoch #38, avg. train loss: 0.00062\n",
      "Step #900, epoch #42, avg. train loss: 0.00063\n",
      "Step #1000, epoch #47, avg. train loss: 0.00064\n",
      "Step #1100, epoch #52, avg. train loss: 0.00062\n",
      "Step #1200, epoch #57, avg. train loss: 0.00064\n",
      "Step #1300, epoch #61, avg. train loss: 0.00063\n",
      "Step #1400, epoch #66, avg. train loss: 0.00064\n",
      "Step #1500, epoch #71, avg. train loss: 0.00062\n",
      "Step #1600, epoch #76, avg. train loss: 0.00063\n",
      "Step #1700, epoch #80, avg. train loss: 0.00063\n",
      "Step #1800, epoch #85, avg. train loss: 0.00061\n",
      "Step #1900, epoch #90, avg. train loss: 0.00062\n",
      "Step #2000, epoch #95, avg. train loss: 0.00064\n",
      "Step #2100, epoch #100, avg. train loss: 0.00062\n",
      "Step #2200, epoch #104, avg. train loss: 0.00062\n",
      "Step #2300, epoch #109, avg. train loss: 0.00065\n",
      "Step #2400, epoch #114, avg. train loss: 0.00064\n",
      "Step #2500, epoch #119, avg. train loss: 0.00060\n",
      "Step #2600, epoch #123, avg. train loss: 0.00062\n",
      "Step #2700, epoch #128, avg. train loss: 0.00067\n",
      "Step #2800, epoch #133, avg. train loss: 0.00059\n",
      "Step #2900, epoch #138, avg. train loss: 0.00062\n",
      "Step #3000, epoch #142, avg. train loss: 0.00064\n",
      "Step #3100, epoch #147, avg. train loss: 0.00060\n",
      "Step #3200, epoch #152, avg. train loss: 0.00065\n",
      "Step #3300, epoch #157, avg. train loss: 0.00061\n",
      "Step #3400, epoch #161, avg. train loss: 0.00063\n",
      "Step #3500, epoch #166, avg. train loss: 0.00061\n",
      "Step #3600, epoch #171, avg. train loss: 0.00062\n",
      "Step #3700, epoch #176, avg. train loss: 0.00061\n",
      "Step #3800, epoch #180, avg. train loss: 0.00061\n",
      "Step #3900, epoch #185, avg. train loss: 0.00059\n",
      "Step #4000, epoch #190, avg. train loss: 0.00062\n",
      "Step #4100, epoch #195, avg. train loss: 0.00059\n",
      "Step #4200, epoch #200, avg. train loss: 0.00060\n",
      "Step #4300, epoch #204, avg. train loss: 0.00060\n",
      "Step #4400, epoch #209, avg. train loss: 0.00057\n",
      "Step #4500, epoch #214, avg. train loss: 0.00063\n",
      "Step #4600, epoch #219, avg. train loss: 0.00060\n",
      "Step #4700, epoch #223, avg. train loss: 0.00062\n",
      "Step #4800, epoch #228, avg. train loss: 0.00058\n",
      "Step #4900, epoch #233, avg. train loss: 0.00060\n",
      "Step #5000, epoch #238, avg. train loss: 0.00060\n",
      "Step #5100, epoch #242, avg. train loss: 0.00058\n",
      "Step #5200, epoch #247, avg. train loss: 0.00061\n",
      "Step #5300, epoch #252, avg. train loss: 0.00058\n",
      "Step #5400, epoch #257, avg. train loss: 0.00061\n",
      "Step #5500, epoch #261, avg. train loss: 0.00060\n",
      "Step #5600, epoch #266, avg. train loss: 0.00059\n",
      "Step #5700, epoch #271, avg. train loss: 0.00063\n",
      "Step #5800, epoch #276, avg. train loss: 0.00058\n",
      "Step #5900, epoch #280, avg. train loss: 0.00059\n",
      "Step #6000, epoch #285, avg. train loss: 0.00061\n",
      "Step #6100, epoch #290, avg. train loss: 0.00061\n",
      "Step #6200, epoch #295, avg. train loss: 0.00059\n",
      "Step #6300, epoch #300, avg. train loss: 0.00060\n",
      "Step #6400, epoch #304, avg. train loss: 0.00062\n",
      "Step #6500, epoch #309, avg. train loss: 0.00058\n",
      "Step #6600, epoch #314, avg. train loss: 0.00059\n",
      "Step #6700, epoch #319, avg. train loss: 0.00061\n",
      "Step #6800, epoch #323, avg. train loss: 0.00062\n",
      "Step #6900, epoch #328, avg. train loss: 0.00062\n",
      "Step #7000, epoch #333, avg. train loss: 0.00058\n",
      "0.968069828162\n",
      "Initialization\n",
      "Initilization\n",
      "Gen. 0 (0.00%): Max/Min/Avg Fitness(Raw) [1.42(3.10)/1.04(0.06)/1.18(1.18)]\n",
      "Gen. 10 (33.33%): Max/Min/Avg Fitness(Raw) [1.70(5.37)/1.33(0.24)/1.42(1.42)]\n",
      "Gen. 20 (66.67%): Max/Min/Avg Fitness(Raw) [2.01(6.94)/1.60(0.42)/1.68(1.68)]\n",
      "Gen. 30 (100.00%): Max/Min/Avg Fitness(Raw) [6.24(7.93)/3.97(1.98)/5.20(5.20)]\n",
      "Total time elapsed: 0.718 seconds.\n",
      "Step #100, epoch #4, avg. train loss: 2.62106\n",
      "Step #200, epoch #9, avg. train loss: 0.00137\n",
      "Step #300, epoch #14, avg. train loss: 0.00099\n",
      "Step #400, epoch #19, avg. train loss: 0.00091\n",
      "Step #500, epoch #23, avg. train loss: 0.00079\n",
      "Step #600, epoch #28, avg. train loss: 0.00071\n",
      "Step #700, epoch #33, avg. train loss: 0.00069\n",
      "Step #800, epoch #38, avg. train loss: 0.00064\n",
      "Step #900, epoch #42, avg. train loss: 0.00065\n",
      "Step #1000, epoch #47, avg. train loss: 0.00065\n",
      "Step #1100, epoch #52, avg. train loss: 0.00063\n",
      "Step #1200, epoch #57, avg. train loss: 0.00064\n",
      "Step #1300, epoch #61, avg. train loss: 0.00064\n",
      "Step #1400, epoch #66, avg. train loss: 0.00065\n",
      "Step #1500, epoch #71, avg. train loss: 0.00063\n",
      "Step #1600, epoch #76, avg. train loss: 0.00064\n",
      "Step #1700, epoch #80, avg. train loss: 0.00063\n",
      "Step #1800, epoch #85, avg. train loss: 0.00062\n",
      "Step #1900, epoch #90, avg. train loss: 0.00063\n",
      "Step #2000, epoch #95, avg. train loss: 0.00065\n",
      "Step #2100, epoch #100, avg. train loss: 0.00063\n",
      "Step #2200, epoch #104, avg. train loss: 0.00062\n",
      "Step #2300, epoch #109, avg. train loss: 0.00065\n",
      "Step #2400, epoch #114, avg. train loss: 0.00064\n",
      "Step #2500, epoch #119, avg. train loss: 0.00060\n",
      "Step #2600, epoch #123, avg. train loss: 0.00062\n",
      "Step #2700, epoch #128, avg. train loss: 0.00067\n",
      "Step #2800, epoch #133, avg. train loss: 0.00059\n",
      "Step #2900, epoch #138, avg. train loss: 0.00062\n",
      "Step #3000, epoch #142, avg. train loss: 0.00063\n",
      "Step #3100, epoch #147, avg. train loss: 0.00060\n",
      "Step #3200, epoch #152, avg. train loss: 0.00064\n",
      "Step #3300, epoch #157, avg. train loss: 0.00061\n",
      "Step #3400, epoch #161, avg. train loss: 0.00063\n",
      "Step #3500, epoch #166, avg. train loss: 0.00061\n",
      "Step #3600, epoch #171, avg. train loss: 0.00062\n",
      "Step #3700, epoch #176, avg. train loss: 0.00061\n",
      "Step #3800, epoch #180, avg. train loss: 0.00062\n",
      "Step #3900, epoch #185, avg. train loss: 0.00060\n",
      "Step #4000, epoch #190, avg. train loss: 0.00064\n",
      "Step #4100, epoch #195, avg. train loss: 0.00060\n",
      "Step #4200, epoch #200, avg. train loss: 0.00061\n",
      "Step #4300, epoch #204, avg. train loss: 0.00061\n",
      "Step #4400, epoch #209, avg. train loss: 0.00058\n",
      "Step #4500, epoch #214, avg. train loss: 0.00065\n",
      "Step #4600, epoch #219, avg. train loss: 0.00060\n",
      "Step #4700, epoch #223, avg. train loss: 0.00064\n",
      "Step #4800, epoch #228, avg. train loss: 0.00059\n",
      "Step #4900, epoch #233, avg. train loss: 0.00059\n",
      "Step #5000, epoch #238, avg. train loss: 0.00060\n",
      "Step #5100, epoch #242, avg. train loss: 0.00057\n",
      "Step #5200, epoch #247, avg. train loss: 0.00060\n",
      "Step #5300, epoch #252, avg. train loss: 0.00058\n",
      "Step #5400, epoch #257, avg. train loss: 0.00060\n",
      "Step #5500, epoch #261, avg. train loss: 0.00059\n",
      "Step #5600, epoch #266, avg. train loss: 0.00058\n",
      "Step #5700, epoch #271, avg. train loss: 0.00063\n",
      "Step #5800, epoch #276, avg. train loss: 0.00058\n",
      "Step #5900, epoch #280, avg. train loss: 0.00059\n",
      "Step #6000, epoch #285, avg. train loss: 0.00059\n",
      "Step #6100, epoch #290, avg. train loss: 0.00059\n",
      "Step #6200, epoch #295, avg. train loss: 0.00059\n",
      "Step #6300, epoch #300, avg. train loss: 0.00058\n",
      "Step #6400, epoch #304, avg. train loss: 0.00060\n",
      "Step #6500, epoch #309, avg. train loss: 0.00056\n",
      "Step #6600, epoch #314, avg. train loss: 0.00057\n",
      "Step #6700, epoch #319, avg. train loss: 0.00059\n",
      "Step #6800, epoch #323, avg. train loss: 0.00059\n",
      "Step #6900, epoch #328, avg. train loss: 0.00061\n",
      "Step #7000, epoch #333, avg. train loss: 0.00058\n",
      "0.966323973341\n",
      "1 loops, best of 3: 10.6 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "main_GABPNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Multilayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Dropout, Input\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_nn.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
