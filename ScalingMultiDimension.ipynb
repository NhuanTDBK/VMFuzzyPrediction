{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scaling.ProactiveSLA import ProactiveSLA\n",
      "from utils.load_result import *\n",
      "from utils.GraphUtil import *\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scaling.ProactiveManager import ProactiveManager\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sla_violate(file_name, broker):\n",
      "    ypred, ytrue = load_file(file_name)\n",
      "    \n",
      "    cpu_pred, cpu_test = np.maximum(ypred[:,0],0), ytrue[:,0]\n",
      "    ram_pred, ram_test = np.maximum(ypred[:,1],0), ytrue[:,1]\n",
      "    \n",
      "    resource_used = np.array(zip(*(cpu_test,ram_test)))\n",
      "    resource_predict = np.array(zip(*(cpu_pred,ram_pred)))\n",
      "    \n",
      "    number_of_VMs = np.array(broker.allocate_VMs(resource_used=resource_used,resource_predicted=resource_predict))\n",
      "    \n",
      "    cpu_pred_VMs = number_of_VMs * broker.capacity_VM[0]\n",
      "    ram_pred_VMs = number_of_VMs * broker.capacity_VM[1]\n",
      "    \n",
      "    c = np.array((cpu_test>=cpu_pred_VMs))\n",
      "    d = np.array((ram_test>=ram_pred_VMs))\n",
      "    e = np.array([(x or y) for x,y in zip(c,d)])\n",
      "    return float(len(e[e==True])) * 100/ len(e)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "broker = ProactiveManager(sla=1.5, past_consecutive_values=10)\n",
      "print sla_violate('model_saved/BPNNM_3_1.05107994991.npz',broker)\n",
      "print sla_violate('model_saved/GABPNNM_5_1.05844921083.npz',broker)\n",
      "print sla_violate('model_saved/Fuzzy_BPNNM_4_0.840165384615.npz',broker)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndexError",
       "evalue": "too many indices for array",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-32-917e0bf61656>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msla_violate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_saved/BPNNM_3_1.05107994991.npz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbroker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msla_violate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_saved/GABPNNM_5_1.05844921083.npz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbroker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0msla_violate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_saved/Fuzzy_BPNNM_4_0.840165384615.npz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbroker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-25-922270255e50>\u001b[0m in \u001b[0;36msla_violate\u001b[0;34m(file_name, broker)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mypred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcpu_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mram_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mram_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5.6856187291\n",
        "2.45810055866\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = np.load('model_saved/Fuzzy_BPNNM_4_0.840165384615.npz')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_true = a['y_true']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_true"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "array([ 0.238,  0.229,  0.205,  0.163,  0.175,  0.184,  0.183,  0.18 ,\n",
        "        0.14 ,  0.17 ,  0.29 ,  0.221,  0.356,  0.562,  0.623,  0.644,\n",
        "        0.   ,  0.536,  0.547,  0.729,  0.672,  0.643,  0.791,  0.677,\n",
        "        0.746,  1.718,  1.082,  1.568,  1.106,  1.486,  1.976,  1.467,\n",
        "        1.639,  1.937,  3.172,  2.186,  2.346,  1.331,  1.536,  1.237,\n",
        "        1.521,  1.44 ,  1.776,  2.003,  2.097,  1.966,  2.429,  2.199,\n",
        "        1.284,  1.135,  1.209,  0.929,  1.3  ,  0.98 ,  1.133,  1.92 ,\n",
        "        1.896,  1.497,  0.894,  0.773,  1.017,  1.007,  0.773,  0.744,\n",
        "        0.674,  1.139,  1.322,  1.07 ,  1.021,  0.831,  0.609,  0.886,\n",
        "        1.033,  1.145,  1.075,  1.154,  1.407,  1.21 ,  1.865,  2.65 ,\n",
        "        3.045,  2.438,  1.788,  0.422,  0.19 ,  0.174,  0.178,  0.183,\n",
        "        0.148,  0.131,  0.165,  0.148,  0.12 ,  0.135,  0.156,  0.163,\n",
        "        0.139,  0.161,  0.161,  0.147,  0.213,  0.183,  0.132,  0.175,\n",
        "        0.159,  0.153,  0.145,  0.147,  0.156,  0.157,  0.135,  0.133,\n",
        "        0.191,  0.174,  0.136,  0.183,  0.151,  0.204,  0.214,  0.236,\n",
        "        0.203,  0.153,  0.179,  0.219,  0.175,  0.174,  0.177,  0.166,\n",
        "        0.16 ,  0.124,  0.144,  0.251,  0.193,  0.127,  0.168,  0.147,\n",
        "        0.15 ,  0.171,  0.144,  0.102,  0.108,  0.14 ,  0.275,  0.231,\n",
        "        0.176,  0.26 ,  0.237,  0.142,  0.138,  0.139,  0.137,  0.171,\n",
        "        0.192,  0.154,  0.16 ,  0.27 ,  0.243,  0.244,  0.191,  0.172,\n",
        "        0.196,  0.229,  0.166,  0.159,  0.163,  0.162,  0.153,  0.184,\n",
        "        0.294,  0.315,  0.217,  0.175,  0.179,  0.184,  0.26 ,  0.214,\n",
        "        0.177,  0.184,  0.181,  0.182,  0.194,  0.174,  0.278,  0.306,\n",
        "        0.499,  0.515,  0.223,  0.194,  0.162,  0.18 ,  0.179,  0.232,\n",
        "        0.238,  0.406,  0.284,  0.192,  0.221,  0.219,  0.255,  0.17 ,\n",
        "        0.131,  0.142,  0.178,  0.159,  0.169,  0.168,  0.186,  0.182,\n",
        "        0.182,  0.304,  0.279,  0.363,  0.34 ,  0.307,  0.244,  0.184,\n",
        "        0.183,  0.163,  0.183,  0.158,  0.176,  0.157,  0.14 ,  0.154,\n",
        "        0.153,  0.143,  0.193,  0.168,  0.205,  0.196,  0.175,  0.177,\n",
        "        0.161,  0.262,  0.253,  0.141,  0.18 ,  0.504,  0.438,  0.231,\n",
        "        0.168,  0.163,  0.163,  0.174,  0.169,  0.17 ,  0.161,  0.159,\n",
        "        0.169,  0.164,  0.148,  0.164,  0.189,  0.148,  0.175,  0.17 ,\n",
        "        0.156,  0.174,  0.149,  0.127,  0.096,  0.17 ,  0.17 ,  0.176,\n",
        "        0.138,  0.106,  0.17 ,  0.183,  0.171,  0.087,  0.145,  0.144,\n",
        "        0.161,  0.149,  0.146,  0.139,  0.147,  0.147,  0.136,  0.146,\n",
        "        0.128,  0.136,  0.147,  0.149,  0.158,  0.212,  0.262,  0.158,\n",
        "        0.165,  0.155,  0.164,  0.166,  0.145,  0.135,  0.258,  0.211,\n",
        "        0.144,  0.143,  0.15 ,  0.158,  0.26 ,  0.204,  0.241,  0.25 ,\n",
        "        0.424,  0.38 ,  0.205,  0.193,  0.199,  0.278,  0.207,  0.194,\n",
        "        0.171,  0.132,  0.28 ,  0.304,  0.206,  0.173,  0.142,  0.157,\n",
        "        0.122,  0.135,  0.182,  0.299,  0.249,  0.41 ,  0.345,  0.175,\n",
        "        0.175,  0.254,  0.211,  0.211,  0.18 ,  0.195,  0.195,  0.29 ,\n",
        "        0.363,  0.312,  0.22 ,  0.263,  0.22 ,  0.21 ,  0.202,  0.213,\n",
        "        0.317,  0.302,  0.176,  0.19 ,  0.18 ,  0.161,  0.169,  0.178,\n",
        "        0.168,  0.42 ,  0.439,  0.292,  0.234,  0.204,  0.189,  0.174,\n",
        "        0.206,  0.253,  0.324,  0.254,  0.199,  0.205,  0.194,  0.171,\n",
        "        0.248,  0.433,  0.355,  0.191,  0.174,  0.181,  0.155,  0.168,\n",
        "        0.163,  0.177,  0.126,  0.143,  0.107,  0.127,  0.208,  0.142,\n",
        "        0.192,  0.176,  0.185,  0.165,  0.152,  0.163,  0.161,  0.157,\n",
        "        0.171,  0.165,  0.164,  0.152,  0.389,  0.361,  0.991,  0.757,\n",
        "        0.349,  0.349,  0.257,  0.214,  0.179,  0.184,  0.18 ,  0.176,\n",
        "        0.381,  0.375,  0.168,  0.153,  0.173,  0.152,  0.197,  0.176,\n",
        "        0.188,  0.133,  0.155,  0.923,  0.682,  0.204,  0.171,  0.16 ,\n",
        "        0.157,  0.166,  0.129,  0.163,  0.147,  0.156,  0.19 ,  0.31 ,\n",
        "        0.227,  0.186,  0.148,  0.206,  0.158,  0.169,  0.134,  0.119,\n",
        "        0.113,  0.157,  0.16 ,  0.   ,  0.161,  0.18 ,  0.178,  0.249,\n",
        "        0.25 ,  0.226,  0.379,  0.326,  0.196,  0.175,  0.188,  0.206,\n",
        "        0.246,  0.212,  0.19 ,  0.181,  0.164,  0.179,  0.161,  0.304,\n",
        "        0.442,  0.334,  0.422,  0.378,  0.541,  0.492,  0.286,  0.184,\n",
        "        0.209,  0.323,  1.064,  1.012,  0.688,  0.416,  0.327,  0.289,\n",
        "        0.187,  0.627,  2.255,  2.587,  2.5  ,  3.022,  3.046,  3.563,\n",
        "        2.637,  1.886,  3.393,  2.714,  2.825,  3.228,  2.227,  0.632,\n",
        "        0.261,  0.237,  0.266,  0.323,  0.254,  0.231,  0.181,  0.186,\n",
        "        0.197,  0.198,  0.187,  0.164,  0.176,  0.21 ,  0.176,  0.174,\n",
        "        0.13 ,  0.171,  0.197,  0.207,  0.201,  0.164,  0.188,  0.195,\n",
        "        0.184,  0.207,  0.193,  0.178,  0.26 ,  0.236,  0.165,  0.203,\n",
        "        0.219,  0.218,  0.208,  0.222,  0.161,  0.181,  0.204,  0.208,\n",
        "        0.204,  0.178,  0.187,  0.19 ,  0.187,  0.161,  0.178,  0.162,\n",
        "        0.189,  0.184,  0.169,  0.171,  0.202,  0.179,  0.2  ,  0.185,\n",
        "        0.234,  0.2  ,  0.182,  0.216,  0.194,  0.159,  0.178,  0.124,\n",
        "        0.119,  0.157,  0.172,  0.191,  0.18 ,  0.2  ,  0.223,  0.216,\n",
        "        0.17 ,  0.172,  0.189,  0.166,  0.156,  0.186,  0.266,  0.264,\n",
        "        0.237,  0.193,  0.193,  0.177,  0.176,  0.189,  0.162,  0.174,\n",
        "        0.169,  0.176,  0.197,  0.183,  0.175,  0.174,  0.18 ,  0.196,\n",
        "        0.199,  0.329,  0.289,  0.195,  0.201,  0.203,  0.21 ,  0.23 ,\n",
        "        0.219,  0.161,  0.165,  0.178,  0.177,  0.205,  0.165,  0.181,\n",
        "        0.184,  0.164,  0.175,  0.182,  0.195,  0.197,  0.185,  0.165,\n",
        "        0.16 ,  0.17 ,  0.194,  0.176,  0.162,  0.119,  0.133,  0.154,\n",
        "        0.163,  0.174,  0.126,  0.356,  0.311,  0.145,  0.16 ,  0.148,\n",
        "        0.325,  0.263,  0.153,  0.17 ,  0.165,  0.199,  0.191,  0.163,\n",
        "        0.177,  0.157,  0.249,  0.314,  0.314,  0.257,  0.176,  0.195,\n",
        "        0.182,  0.143,  0.166,  0.183,  0.177,  0.148,  0.358,  0.25 ,\n",
        "        0.22 ,  1.713,  3.381,  4.967,  5.397,  5.464,  5.545,  3.415,\n",
        "        2.887,  0.435,  0.366,  0.198,  0.169,  0.166,  0.188,  0.187,\n",
        "        0.193,  0.2  ,  0.186,  0.202,  0.17 ,  0.175,  0.17 ,  0.159,\n",
        "        0.168,  0.141,  0.113,  0.136,  0.191,  0.18 ,  0.186,  0.181,\n",
        "        0.185,  0.163,  0.178,  0.162,  0.167,  0.164,  0.205,  0.172,\n",
        "        0.181,  0.217,  0.215,  0.164,  0.187,  0.195,  0.181,  0.167,\n",
        "        0.19 ,  0.174,  0.167,  0.169,  0.161,  0.172,  0.166,  0.153,\n",
        "        0.164,  0.179,  0.183,  0.155,  0.184,  0.154,  0.14 ,  0.166,\n",
        "        0.195,  0.157,  0.139,  0.226,  0.293,  0.26 ,  0.207,  0.184,\n",
        "        0.184,  0.148,  0.174,  0.176,  0.148,  0.186,  0.161,  0.162,\n",
        "        0.288,  0.264,  0.386,  0.423,  0.221,  0.175,  0.174,  0.175,\n",
        "        0.196,  0.206,  0.193,  0.155,  0.174,  0.182,  0.155,  0.168,\n",
        "        0.173,  0.18 ,  0.165,  0.343,  0.341,  0.906,  0.644,  0.297,\n",
        "        0.168,  0.181,  0.18 ,  0.162,  0.156,  0.18 ,  0.191,  0.177,\n",
        "        0.17 ,  0.174,  0.169,  0.167,  0.295,  0.286,  0.17 ,  0.169,\n",
        "        0.337,  0.306,  0.168,  0.167,  0.148,  0.189,  0.168,  0.158,\n",
        "        0.181,  0.168,  0.338,  0.329,  0.369,  0.26 ,  0.171,  0.176,\n",
        "        0.155,  0.15 ,  0.178,  0.162,  0.199,  0.201,  0.178,  0.156,\n",
        "        0.143,  0.112,  0.12 ,  0.135,  0.165,  0.187,  0.161,  0.134,\n",
        "        0.179,  0.177,  0.175,  0.163,  0.151,  0.15 ,  0.159,  0.183,\n",
        "        0.209,  0.191,  0.167,  0.164,  0.162,  0.168,  0.164,  0.163,\n",
        "        0.177,  0.166,  0.162,  0.183,  0.153,  0.163,  0.169,  0.158,\n",
        "        0.164,  0.177,  0.168,  0.148,  0.149,  0.189,  0.203,  0.157,\n",
        "        0.171,  0.18 ,  0.165,  0.167,  0.178,  0.19 ,  0.153,  0.168,\n",
        "        0.187,  0.154,  0.16 ,  0.145,  0.182,  0.182,  0.173,  0.156,\n",
        "        0.172,  0.141,  0.17 ,  0.178,  0.131,  0.123,  0.126,  0.233,\n",
        "        0.23 ,  0.251,  0.192,  0.184,  0.199,  0.182,  0.188,  0.151,\n",
        "        0.165,  0.154,  0.163,  0.176,  0.162,  0.202,  0.189,  0.171,\n",
        "        0.279,  0.322,  0.329,  0.203,  0.212,  0.209,  0.172,  0.195,\n",
        "        0.209,  0.206,  0.373,  0.29 ,  0.184,  0.17 ,  0.193,  0.155,\n",
        "        0.154])"
       ]
      }
     ],
     "prompt_number": 36
    }
   ],
   "metadata": {}
  }
 ]
}